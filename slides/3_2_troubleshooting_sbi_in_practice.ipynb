{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6795b1fa",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sbi.inference import SNLE, SNPE, prepare_for_sbi, simulate_for_sbi\n",
    "from sbi.simulators.linear_gaussian import (\n",
    "    linear_gaussian,\n",
    "    samples_true_posterior_linear_gaussian_uniform_prior,\n",
    ")\n",
    "import torch\n",
    "from sbi.utils import BoxUniform\n",
    "import matplotlib.pyplot as plt\n",
    "from sbi.analysis import pairplot\n",
    "import sbibm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1498000d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What could go wrong (and how to fix it)\n",
    "\n",
    "## 1. Session: Stay in the SBI bubble: \n",
    "\n",
    "- assume valid simulator and prior\n",
    "\n",
    "<img src=\"figures/bubble_zacktionman_Flickr.jpg\" align=\"center\" alt=\"beadexample\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c538677",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What could go wrong (and how to fix it)\n",
    "\n",
    "## 2. Session: the full Bayesian workflow\n",
    "\n",
    "- model building, prior checks -> inference -> posterior checks\n",
    "<img src=\"figures/what-if-i-told-you-this-is-not-fully-bayesian.jpg\" align=\"center\" alt=\"beadexample\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2c5faf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What could go wrong (and how to fix it)\n",
    "\n",
    "## 1. Session: Stay in the SBI bubble: assume valid simulator and prior\n",
    "\n",
    "<img src=\"figures/bubble_zacktionman_Flickr.jpg\" align=\"center\" alt=\"beadexample\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90866eba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"figures/bubble_zacktionman_Flickr.jpg\" align=\"right\" width=\"100\"/>\n",
    "\n",
    "## What could go wrong?\n",
    "\n",
    "- the density estimator is off\n",
    "\n",
    "- MCMC samples are off (likelihood based methods)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f2f4ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"figures/bubble_zacktionman_Flickr.jpg\" align=\"right\" width=\"100\"/>\n",
    "\n",
    "## What could be the reason?\n",
    "\n",
    "1) training not converged \"properly\" / too little training data\n",
    "    \n",
    "2) density estimator lacks flexibility\n",
    "\n",
    "3) summary statistics (or embedding net) not informative (not discussed today)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47088dcb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"figures/bubble_zacktionman_Flickr.jpg\" align=\"right\" width=\"100\"/>\n",
    "\n",
    "## How to diagnose it?\n",
    "\n",
    "- training and inference logs:\n",
    "    - validaton and training loss convergence\n",
    "    - more to come... (any ideas?)\n",
    "    - (MCMC convergence statistics)\n",
    "\n",
    "- posterior predictive checks\n",
    "    - sample from posterior and simulate\n",
    "    - compare to $x_o$\n",
    "    \n",
    "- simulation-based calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d70871",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem 1: Training not converged\n",
    "\n",
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdf3274",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Gaussian simulator\n",
    "def simulator(theta, scale=0.5):\n",
    "    # Sample from standard normal, shift with mean.\n",
    "    return scale * torch.randn(theta.shape) + theta\n",
    "\n",
    "num_dim = 3\n",
    "simulator_scale = 0.1\n",
    "num_samples = 1000\n",
    "# Uniform prior.\n",
    "prior = BoxUniform(-5 * torch.ones(num_dim), 5 * torch.ones(num_dim))\n",
    "x_o = torch.ones(1, num_dim)\n",
    "# True posterior\n",
    "true_samples = simulator_scale * torch.randn(num_samples, num_dim) + x_o "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f54238",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# run simulations\n",
    "num_simulations = 20  # Little training data.\n",
    "theta = prior.sample((num_simulations,))\n",
    "x = simulator(theta, scale=simulator_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe9ab79",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# run inference\n",
    "inferer = SNPE(prior, density_estimator=\"mdn\").append_simulations(theta, x)\n",
    "density_estimator = inferer.train()\n",
    "posterior = inferer.build_posterior()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297f7370",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SBI posterior is off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164472e6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Draw posterior samples and plot 1 and 2-D marginals.\n",
    "posterior_samples = posterior.sample((num_samples,), x=x_o)\n",
    "pairplot([posterior_samples, true_samples], upper=\"scatter\", limits=[[-5, 5]], figsize=(8, 8));\n",
    "plt.legend(['sbi-posterior' ,'true-posterior']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d1ed40",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### How can we detect this? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98331e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(inferer.summary[\"train_log_probs\"])\n",
    "plt.plot(inferer.summary[\"validation_log_probs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb47025",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "### 1) SBI training logs with Tensorboard\n",
    "\n",
    "- Running tensorboard: in the terminal and in the same folder as this notebook run:\n",
    "\n",
    "`tensorboard --logdir sbi-logs/`\n",
    "\n",
    "- This will open a Tensorboard on a localhost, usually http://localhost:6006/\n",
    "\n",
    "- Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af45393",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tensorboard demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66db1a7f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "### 2) Posterior predictive checks\n",
    "\n",
    "- General idea: samples from the posterior should reproduce the observed data $x_o$\n",
    "    - plus simulator noise\n",
    "\n",
    "- Samples from the posterior plugged into the simulator should cluster around $x_o$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e60b23",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example: Posterior predictive checks with true posterior samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8947549f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Simulate with true posterior samples from above.\n",
    "posterior_predictive_samples = simulator(true_samples)\n",
    "# Plot on top of x_o\n",
    "pairplot([posterior_predictive_samples], upper=\"scatter\", points_colors=\"k\", points=x_o, \n",
    "         limits=[[-5, 5]], labels=[rf\"$x_{ii}$\" for ii in range(num_dim)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75bcfae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practical 1:\n",
    "\n",
    "1. Run the inference from the previous example.\n",
    "2. Start Tensorboard and inspect the training logs\n",
    "    - have a look training log probs and validation log probs\n",
    "    - what do you observe? did the training converge properly?\n",
    "3. Run posterior predictive checks with the trained density estimator.\n",
    "4. Change the training settings and re-run the inference. \n",
    "5. Repeat the checks, did it help?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4dbb69",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem 2: Lack of flexibility of the density estimator\n",
    "\n",
    "- SBI offers different types of density estimators: \n",
    "    - mixture density networks (of Gaussians) (MDN)\n",
    "    - normalizing flows\n",
    "\n",
    "- MDN are fast in training, sampling and evaluation\n",
    "- Flows are more flexible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67863b5",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Normalizing flows\n",
    "\n",
    "<img src=\"figures/nutshell_Kletr-Shutterstock.jpg\" align=\"left\" alt=\"beadexample\" width=\"130\"/> \n",
    "\n",
    "- transform a simple base distribution to a complex target distribution\n",
    "\n",
    "- transforms can be trained with NNs (under certain assumptions)\n",
    "\n",
    "- concatenating transforms -> powerful (conditional) density estimator\n",
    "\n",
    "- implemented in `sbi`: \n",
    "    - masked autoregressive flows (\"MAF\")\n",
    "    - neural spline flows (\"NSF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77693a63",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 2: Inference on the two-moon task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc35e5d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load two-moon task from sbi-benchmark package\n",
    "import sbibm\n",
    "task = sbibm.get_task(\"two_moons\")\n",
    "simulator = task.get_simulator()\n",
    "prior = task.get_prior_dist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4220d9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Simulate\n",
    "num_simulations = 10000\n",
    "num_samples = 1000\n",
    "theta = prior.sample((num_simulations,))\n",
    "x = simulator(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07897a1b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# run inference with MDN\n",
    "inferer = SNPE(prior, density_estimator=\"mdn\").append_simulations(theta, x)\n",
    "density_estimator = inferer.train()\n",
    "posterior = inferer.build_posterior(density_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d98e773",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### MDN posterior fails to learn the two moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8051dc6a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Draw posterior samples and plot 1 and 2-D marginals.\n",
    "x_o = task.get_observation(1)\n",
    "mdn_samples = posterior.sample((num_samples,), x=x_o)\n",
    "true_samples = task.get_reference_posterior_samples(1)[:num_samples,]\n",
    "pairplot([mdn_samples, true_samples], upper=\"scatter\", limits=[[-1, 1]], figsize=(7, 7));\n",
    "plt.legend([\"mdn-posterior\", \"true-posterior\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5267dfc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practical 2: Changing density estimators\n",
    "\n",
    "### Tasks\n",
    "1. Change the density estimator to a flow ([documentation](https://www.mackelab.org/sbi/tutorial/04_density_estimators/)) and train again.\n",
    "2. Compare to the reference posterior samples (obtained from `sbibm`) using `pairplot`.\n",
    "3. Compare the different density estimators in terms of posterior predictive samples. \n",
    "4. [Optional] The `density_estimator` argument takes a `string` or a function. By passing a `string` you get a density estimator with default settings, by passing a function you can pass your custom density estimator. Have a look at `sbi.utils.get_nn_models` to see how to build such a customised density estimator using the function `posterior_nn(...)` or `likelihood_nn(...)`. [More information](https://www.mackelab.org/sbi/tutorial/04_density_estimators/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888be1de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem 3: Leakage in multi-round inference\n",
    "\n",
    "- **multi-round inference**: in every new round we simulate new training data not from the prior, but from the recent posterior estimate\n",
    "- this requires some correction and algorithmic sugar (see SNPE [A](https://proceedings.neurips.cc/paper/2016/file/6aca97005c68f1206823815f66102863-Paper.pdf), [B](https://openreview.net/forum?id=Jwgr1P_AioF) and [C](http://proceedings.mlr.press/v97/greenberg19a) papers)\n",
    "- but can improve data efficiency (see `sbibm` benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc78174",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Problem 3: Leakage in multi-round inference\n",
    "\n",
    "- SNPE-C is the current method of choice: stable training, ability to use of flows!\n",
    "- but it comes with a drawback as well:\n",
    "\n",
    "- for complex problems with bounded priors posterior mass tends to leak outside of the prior bounds\n",
    "- this can get extreme: with 99,9% of the mass leaking out (warning in `sbi`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1e9d22",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Solution to problem 3:\n",
    "\n",
    "- sample posterior with MCMC or classic rejection sampling.\n",
    "- use likelihood based approaches (SNLE, SNRE) if possible (depends on kind of data).\n",
    "- use single round inference ;-) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b535515a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem 4: MCMC \n",
    "- we use MCMC to obtain posterior samples after learning a synthetic likelihood (SNLE) or likelihood ratio (SNRE)\n",
    "- MCMC is established and reliable but still can get tricky for complex distributions and in high-dimensional spaces\n",
    "\n",
    "\n",
    "- MCMC exploring high-dimensional spaces:\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/jIhNUBuRD2SKA4TrrU/giphy-downsized-large.gif?cid=ecf05e47a7kphsyhpj1g6smvuuc19scoyklxo7cv1krlgwc9&rid=giphy-downsized-large.gif&ct=g\" align=\"right\" alt=\"map\"  width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e122f854",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- When the prior is bounded MCMC can get stuck in \"corners\" of the parameter space\n",
    "- Solution: \n",
    "    - run MCMC with parameters transformed to unbounded space \n",
    "        - already implemented in `sbi`\n",
    "    - check MCMC diagnostics (e.g., chain autocorrelation) \n",
    "        - not yet implemented\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3271df44",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Practical: Running MCMC\n",
    "\n",
    "1) Select an inference problem from above (optional: choose a task from the `sbibm` package)\n",
    "\n",
    "2) Train a density estimator using `SNLE` or `SNRE`. \n",
    "\n",
    "3) Generate posterior samples using `posterior_object.sample(...)`\n",
    "\n",
    "- have a look at the `sbi` API and play around with changing the `mcmc_method` (\"slice_np\", \"slice_np_vectorized\", \"slice\", \"hmc\") and `mcmc_settings` (`num_chains`, `thin`, `init_strategy`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1502da",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Your code:\n",
    "# ...\n",
    "# posterior_samples = posterior.sample(?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180522a3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MCMC Practical: questions & comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bafb80",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discussion of open questions\n",
    "\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34212028",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "### Further reading\n",
    "- benchmarking sbi paper: [Lueckmann et al. 2021](http://proceedings.mlr.press/v130/lueckmann21a.html)\n",
    "- normalizing flows review: [Papamakarios et al. 2019](https://arxiv.org/abs/1912.02762)\n",
    "- review on MCMC in practice: [Hogg and Foreman-Mackey 2016](https://iopscience.iop.org/article/10.3847/1538-4365/aab76e/meta)\n",
    "\n",
    "### Figures\n",
    "- bubble: zacktionman Flickr\n",
    "- nutshell: Kletr Shutterstock\n",
    "\n",
    "- GIFs from Giphy\n",
    "- American woodstock GIF: @Martin_Trapp on Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a103c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
